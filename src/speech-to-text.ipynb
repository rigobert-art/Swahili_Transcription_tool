{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename          object\n",
      "transcription     object\n",
      "filepath          object\n",
      "sample_rate        int64\n",
      "duration         float64\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rigobert Kiata\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SWH-05-20101106_16k-emission_swahili_05h30_-_0...</td>\n",
       "      <td>rais wa tanzania jakaya mrisho kikwete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SWH-05-20101106_16k-emission_swahili_05h30_-_0...</td>\n",
       "      <td>yanayo andaliwa nami pendo pondo idhaa ya kisw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SWH-05-20101106_16k-emission_swahili_05h30_-_0...</td>\n",
       "      <td>inayokutangazia moja kwa moja kutoka jijini da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SWH-05-20101106_16k-emission_swahili_05h30_-_0...</td>\n",
       "      <td>juma hili bara la afrika limeshuhudia raia wa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SWH-05-20101106_16k-emission_swahili_05h30_-_0...</td>\n",
       "      <td>wakipiga kura ya maoni ilikufanya mabadiliko ya</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename   \n",
       "0  SWH-05-20101106_16k-emission_swahili_05h30_-_0...  \\\n",
       "1  SWH-05-20101106_16k-emission_swahili_05h30_-_0...   \n",
       "2  SWH-05-20101106_16k-emission_swahili_05h30_-_0...   \n",
       "3  SWH-05-20101106_16k-emission_swahili_05h30_-_0...   \n",
       "4  SWH-05-20101106_16k-emission_swahili_05h30_-_0...   \n",
       "\n",
       "                                       transcription  \n",
       "0             rais wa tanzania jakaya mrisho kikwete  \n",
       "1  yanayo andaliwa nami pendo pondo idhaa ya kisw...  \n",
       "2  inayokutangazia moja kwa moja kutoka jijini da...  \n",
       "3  juma hili bara la afrika limeshuhudia raia wa ...  \n",
       "4    wakipiga kura ya maoni ilikufanya mabadiliko ya  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import wave\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from jiwer import wer\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import librosa\n",
    "import winsound\n",
    "from scipy.io import wavfile\n",
    "import random\n",
    "\n",
    "# loading text files\n",
    "df = pd.read_csv(\"Text/metadata_TZ_.csv\")\n",
    "print(df.dtypes)\n",
    "\n",
    "df = df[['filename','transcription']]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set: 8144\n",
      "Size of the testing set: 2036\n"
     ]
    }
   ],
   "source": [
    "split = int(len(df) * 0.8)\n",
    "df_train_text = df[:split]\n",
    "df_val_text = df[split:]\n",
    "\n",
    "# df_train.describe\n",
    "print(f\"Size of the training set: {len(df_train_text)}\")\n",
    "print(f\"Size of the testing set: {len(df_val_text)}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SETTING UP THE SWAHILI VOCABULARY TO INCLUDE IN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary is: ['', 'a', 'e', 'i', 'o', 'u', 'b', 'c', 'd', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'r', 's', 't', 'v', 'w', 'y', 'z', \"'\", '?', '!', ' '] size =28 \n"
     ]
    }
   ],
   "source": [
    "# The set of characters accepted in the transcription\n",
    "char = [x for x in \"aeioubcdghjklmnprstvwyz'?! \"]\n",
    "\n",
    "# Mapping characters to intergers\n",
    "char_to_num = keras.layers.StringLookup(vocabulary=char, oov_token=\"\")\n",
    "\n",
    "# mapping intergers back to original characters \n",
    "num_to_char = keras.layers.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n",
    "    f\"size ={char_to_num.vocabulary_size()} \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.preprocessing.string_lookup.StringLookup at 0x2699b9156a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_num"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUDIO DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio files succesfully uploaded\n",
      "Waveform shape:  (50240, 1)\n",
      "Sample rate:  16000\n"
     ]
    }
   ],
   "source": [
    "# loading the audio files\n",
    "# Directory\n",
    "folder_path = 'tz_swh_train'\n",
    "\n",
    "wav_paths = os.listdir(folder_path)\n",
    "\n",
    "file_paths = [os.path.join(folder_path, f) for f in wav_paths if f.endswith(\".wav\")]\n",
    "\n",
    "audio_data = tf.data.Dataset.from_tensor_slices(file_paths)\n",
    "\n",
    "# check if file exists\n",
    "if len(file_paths) > 0:\n",
    "    print('Audio files succesfully uploaded')\n",
    "else:\n",
    "    print(\"Files Note loaded\")\n",
    "\n",
    "# function loading the audio files\n",
    "def load_audio(file_path):\n",
    "    audio_binary = tf.io.read_file(file_path)\n",
    "    waveform, sample_rate = tf.audio.decode_wav(audio_binary)\n",
    "    return waveform, sample_rate\n",
    "\n",
    "\n",
    "#  Map the dataset with the load_audio function\n",
    "audio_data = audio_data.map(load_audio)\n",
    "\n",
    "# Print the first audio file\n",
    "for waveform, sample_rate in audio_data.take(1):\n",
    "    print(\"Waveform shape: \", waveform.shape)\n",
    "    print(\"Sample rate: \", sample_rate.numpy())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUDIO PLAYBACK, FIRST 3 AUDIO FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audio_files = os.listdir(folder_path)\n",
    "# Playing the first audio files\n",
    "# Play the first three audio files\n",
    "for i, swahili in enumerate(audio_files):\n",
    "    audio_path = os.path.join(folder_path, swahili)\n",
    "    if swahili.endswith(\".wav\") and i < 1:\n",
    "        sound = os.path.join(folder_path, swahili)\n",
    "        winsound.PlaySound(sound, winsound.SND_FILENAME + winsound.SND_LOOP)\n",
    "        winsound.PlaySound(None, 0)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUDIO PROCESSING VIA TENSORFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An integer scalar Tensor. The window length in samples\n",
    "frame_length = 256\n",
    "# An integer scalar Tensor. The number of samples to step\n",
    "frame_step = 160\n",
    "# An integer scalar Tensor. the size of the FFT to apply\n",
    "# If not provided, uses the smallest power of 2 enclosing frame_length.\n",
    "fft_length = 1024\n",
    "\n",
    "\n",
    "def encode_single_sample(wav_file, label):\n",
    "    wav_file, label = sample\n",
    "    ####################################\n",
    "    ###    Audio processing for tensorflow \n",
    "    ####################################\n",
    "    # 1. Read wav files\n",
    "    file = tf.io.read_file(audio_files + wav_file + audio_data)\n",
    "    # 2. Decode the wav file\n",
    "    audio = tf.audio.decode_wav(file)\n",
    "    \n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    # change the file type to float\n",
    "    audio = tf.cast(audio, tf.float32)\n",
    "    \n",
    "    spectogram = tf.signal.stft(\n",
    "        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n",
    "    )\n",
    "    # we only need the magnitude, which can be derived by applying tf.abs\n",
    "    spectogram = tf.abs(spectogram)\n",
    "    spectogram = tf.math.pow(spectogram, 0.5)\n",
    "    # normalization\n",
    "    means = tf.math.reduce_mean(spectogram, 1, keepdims=True)\n",
    "    stddevs = tf.math.reduce_std(spectogram, 1, keepdims=True)\n",
    "    spectogram = (spectogram - means) / (stddevs + 1e-10)\n",
    "\n",
    "\n",
    "    label = tf.strings.lower(label)\n",
    "    label = tf.string.unicode_split(label, input_encoding=\"UTF-8\")\n",
    "    label = char_to_num(label)\n",
    "    return spectogram, label\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLITTING THE TRAIN AND TEST SET FOR MODEL TRAINING AND TESTING USING TENSORFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the spectrogram parameters\n",
    "frame_length = 0.025\n",
    "frame_stride = 0.010\n",
    "n_fft = 512\n",
    "hop_length = int(frame_stride * 16000)\n",
    "win_length = int(frame_length * 16000)\n",
    "\n",
    "# For audio spliting\n",
    "train_size = 0.8\n",
    "test_size = 0.2\n",
    "\n",
    "random.shuffle(audio_files)\n",
    "split_index = int(test_size * len(audio_files))\n",
    "train_audio_files = audio_files[:split_index]\n",
    "test_audio_files = audio_files[split_index:]\n",
    "\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((df['filename'], df['transcription']))\n",
    "batch_size = 32\n",
    "dataset = ds.batch(batch_size)\n",
    "# print(audio_path)\n",
    "\n",
    "train_spectrograms = []\n",
    "test_spectrograms = []\n",
    "for folder_path in train_audio_files:\n",
    "   rate, data = wavfile.read(os.path.join(folder_path, ' '.join(audio_files)))\n",
    "   spectrogram = tf.signal.stft(data, frame_length=win_length, frame_step=hop_length)\n",
    "   train_spectrograms.append(spectogram)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_TakeDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[147], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(train_dataset)\n\u001b[0;32m      4\u001b[0m spectogram \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m----> 5\u001b[0m spectogram \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39mtrim_zeros(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(spectogram)])\n\u001b[0;32m      6\u001b[0m label \u001b[39m=\u001b[39m batch[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[39m# spectogram\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: iteration over a 0-d array"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8, 5))\n",
    "for batch in train_dataset.take(1):\n",
    "   print(train_dataset)\n",
    "   spectogram = batch[0][0].numpy()\n",
    "   spectogram = np.array([np.trim_zeros(x) for x in np.transpose(spectogram)])\n",
    "   label = batch[1][0]\n",
    "   # spectogram\n",
    "   label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "   ax = plt.subplot(2, 1, 1)\n",
    "   ax.imshow(spectogram, vmax=1)\n",
    "   ax.set_title(label)\n",
    "   ax.axis(\"off\")\n",
    "   # wav\n",
    "   file = tf.io.read_file(audio_files + list(df_train['filename'])[0] + audio_data)\n",
    "   audio = tf.audio.decode_wav(file)\n",
    "   audio = audio.numpy()\n",
    "   ax = plt.subplot(2, 1, 2)\n",
    "   plt.plot(audio)\n",
    "   ax.set_title(\"signal wave\")\n",
    "   ax.set_xlim(0, len(audio))\n",
    "   display.display(display.Audio(np.transpose(audio), rate=16000))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CTCLOSS detection algorithmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CTCLoss(y_true, y_pred):\n",
    "   batch_len = tf.cast(tf.shape(y_true)[0], dtype='int64')\n",
    "   input_length = tf.cast(tf.shape(y_pred)[1], dytpe=\"int64\")\n",
    "   label_length = tf.cast(tf.shape(y_true)[0], dtype='int64')\n",
    "\n",
    "   input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "   label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "   loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "   return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DeepSpeech_2\"\n",
      "______________________________________________________________________________________________________________\n",
      " Layer (type)                                    Output Shape                                Param #          \n",
      "==============================================================================================================\n",
      " input (InputLayer)                              [(None, None, 193)]                         0                \n",
      "                                                                                                              \n",
      " expand_dim (Reshape)                            (None, None, 193, 1)                        0                \n",
      "                                                                                                              \n",
      " conv_1 (Conv2D)                                 (None, None, 97, 32)                        14432            \n",
      "                                                                                                              \n",
      " conv_1_bn (BatchNormalization)                  (None, None, 97, 32)                        128              \n",
      "                                                                                                              \n",
      " conv_1_relu (ReLU)                              (None, None, 97, 32)                        0                \n",
      "                                                                                                              \n",
      " conv_2 (Conv2D)                                 (None, None, 49, 32)                        236544           \n",
      "                                                                                                              \n",
      " conv_2_bn (BatchNormalization)                  (None, None, 49, 32)                        128              \n",
      "                                                                                                              \n",
      " conv_2_relu (ReLU)                              (None, None, 49, 32)                        0                \n",
      "                                                                                                              \n",
      " reshape_5 (Reshape)                             (None, None, 1568)                          0                \n",
      "                                                                                                              \n",
      " bidirectional_1 (Bidirectional)                 (None, None, 1024)                          6395904          \n",
      "                                                                                                              \n",
      " dropout_25 (Dropout)                            (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " bidirectional_2 (Bidirectional)                 (None, None, 1024)                          4724736          \n",
      "                                                                                                              \n",
      " dropout_26 (Dropout)                            (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " bidirectional_3 (Bidirectional)                 (None, None, 1024)                          4724736          \n",
      "                                                                                                              \n",
      " dropout_27 (Dropout)                            (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " bidirectional_4 (Bidirectional)                 (None, None, 1024)                          4724736          \n",
      "                                                                                                              \n",
      " dropout_28 (Dropout)                            (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " bidirectional_5 (Bidirectional)                 (None, None, 1024)                          4724736          \n",
      "                                                                                                              \n",
      " dense_1 (Dense)                                 (None, None, 1024)                          1049600          \n",
      "                                                                                                              \n",
      " dense_1_relu (ReLU)                             (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " dropout_29 (Dropout)                            (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " dense_5 (Dense)                                 (None, None, 29)                            29725            \n",
      "                                                                                                              \n",
      "==============================================================================================================\n",
      "Total params: 26,625,405\n",
      "Trainable params: 26,625,277\n",
      "Non-trainable params: 128\n",
      "______________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(input_dim, output_dim, rnn_layers=5, rnn_units=138):\n",
    "\n",
    "   input_spectogram = layers.Input((None, input_dim), name=\"input\")\n",
    "\n",
    "   x = layers.Reshape((-1, 193, 1), name=\"expand_dim\")(input_spectogram)\n",
    "\n",
    "   x = layers.Conv2D(\n",
    "      filters=32,\n",
    "      kernel_size=[11, 41],\n",
    "      strides=[2, 2],\n",
    "      padding=\"same\",\n",
    "      use_bias=False,\n",
    "      name=\"conv_1\",\n",
    "   )(x)\n",
    "   x = layers.BatchNormalization(name=\"conv_1_bn\")(x)\n",
    "   x = layers.ReLU(name=\"conv_1_relu\")(x)\n",
    "\n",
    "   x = layers.Conv2D(\n",
    "      filters=32,\n",
    "      kernel_size=[11, 21],\n",
    "      strides=[1, 2],\n",
    "      padding=\"same\",\n",
    "      use_bias=False,\n",
    "      name=\"conv_2\",\n",
    "   )(x)\n",
    "   x = layers.BatchNormalization(name=\"conv_2_bn\")(x)\n",
    "   x = layers.ReLU(name=\"conv_2_relu\")(x)\n",
    "   x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n",
    "\n",
    "   for i in range(1, rnn_layers + 1):\n",
    "      recurrent = layers.GRU(\n",
    "         units=rnn_units,\n",
    "         activation=\"tanh\",\n",
    "         recurrent_activation=\"sigmoid\",\n",
    "         use_bias=True,\n",
    "         return_sequences=True,\n",
    "         reset_after=True,\n",
    "         name=f\"gru_{i}\",\n",
    "      )\n",
    "      x = layers.Bidirectional(\n",
    "         recurrent, name=f\"bidirectional_{i}\", merge_mode=\"concat\"\n",
    "      )(x)\n",
    "      if i < rnn_layers:\n",
    "         x = layers.Dropout(rate=0.5)(x)\n",
    "   x = layers.Dense(units=rnn_units * 2, name=\"dense_1\")(x)\n",
    "   x = layers.ReLU(name=\"dense_1_relu\")(x)\n",
    "   x = layers.Dropout(rate=0.5)(x)\n",
    "\n",
    "   output = layers.Dense(units=output_dim + 1, activation=\"softmax\")(x)\n",
    "\n",
    "   model = keras.Model(input_spectogram, output, name=\"DeepSpeech_2\")\n",
    "\n",
    "   opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "   model.compile(optimizer=opt, loss=CTCLoss)\n",
    "   return model\n",
    "\n",
    "\n",
    "# get the model\n",
    "model = build_model(\n",
    "   input_dim = fft_length // 2 + 1,\n",
    "   output_dim = char_to_num.vocabulary_size(),\n",
    "   rnn_units = 512,\n",
    ")\n",
    "model.summary(line_length=110)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_batch_predictions(pred, input_shape):\n",
    "   input_len = np.ones(pred.shape[0]) * input_shape[1]\n",
    "   results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n",
    "   output_text = []\n",
    "   for result in results:\n",
    "      result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
    "      output_text.append(result)\n",
    "   return output_text\n",
    "\n",
    "class CallbackEval(keras.callbacks.Callback):\n",
    "\n",
    "   def __init__(self, dataset):\n",
    "      super().__init__()\n",
    "      self.dataset = dataset\n",
    "   \n",
    "   def on_epoch_end(self, epoch: int, logs=None):\n",
    "      predictions = []\n",
    "      targets = []\n",
    "      for batch in self.dataset:\n",
    "         X, y = batch\n",
    "         batch_predictions = model.predict(X)\n",
    "         batch_predictions = decode_batch_predictions(batch_predictions, X.shape)\n",
    "         predictions.extend(batch_predictions)\n",
    "         for label in y:\n",
    "            label = (\n",
    "               tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "            )\n",
    "            targets.append(label)\n",
    "      wer_score = wer(targets, predictions)\n",
    "      print(\"-\" * 100)\n",
    "      print(f\"word error rate: {wer_score: 4f}\")\n",
    "      print(\"-\" * 100)\n",
    "      for i in np.random.randint(0, len(predictions), 2):\n",
    "         print(f\"Target :  {targets[i]}\")\n",
    "         print(f\"Prediction: {predictions[i]}\")\n",
    "         print(\"-\" * 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin Training Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Rigobert Kiata\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Rigobert Kiata\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Rigobert Kiata\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Rigobert Kiata\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\Rigobert Kiata\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Rigobert Kiata\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\layers\\reshaping\\reshape.py\", line 115, in _fix_unknown_dimension\n        raise ValueError(msg)\n\n    ValueError: Exception encountered when calling layer 'expand_dim' (type Reshape).\n    \n    total size of new array must be unchanged, input_shape = [], output_shape = [-1, 193, 1]\n    \n    Call arguments received by layer 'expand_dim' (type Reshape):\n      • inputs=tf.Tensor(shape=(None,), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[153], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m validation_callback \u001b[39m=\u001b[39m CallbackEval(test_dataset)\n\u001b[0;32m      5\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      7\u001b[0m    train_dataset,\n\u001b[0;32m      8\u001b[0m    validation_data\u001b[39m=\u001b[39;49mtest_dataset,\n\u001b[0;32m      9\u001b[0m    epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m     10\u001b[0m    callbacks\u001b[39m=\u001b[39;49m[validation_callback],\n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Rigobert Kiata\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\RIGOBE~1\\AppData\\Local\\Temp\\__autograph_generated_file67nymxah.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Rigobert Kiata\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Rigobert Kiata\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Rigobert Kiata\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Rigobert Kiata\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\Rigobert Kiata\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Rigobert Kiata\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\layers\\reshaping\\reshape.py\", line 115, in _fix_unknown_dimension\n        raise ValueError(msg)\n\n    ValueError: Exception encountered when calling layer 'expand_dim' (type Reshape).\n    \n    total size of new array must be unchanged, input_shape = [], output_shape = [-1, 193, 1]\n    \n    Call arguments received by layer 'expand_dim' (type Reshape):\n      • inputs=tf.Tensor(shape=(None,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define the number of epochs.\n",
    "epochs = 50\n",
    "# Callback function to check transcription on the val set.\n",
    "validation_callback = CallbackEval(test_dataset)\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "   train_dataset,\n",
    "   validation_data=test_dataset,\n",
    "   epochs=epochs,\n",
    "   callbacks=[validation_callback],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "After applying the transformation, each reference should be a non-empty list of strings, with each string being a single word.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m       label \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mstrings\u001b[39m.\u001b[39mreduce_join(num_to_char(label))\u001b[39m.\u001b[39mnumpy\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m       targets\u001b[39m.\u001b[39mappend(label)\n\u001b[1;32m---> 11\u001b[0m wer_score \u001b[39m=\u001b[39m wer(targets, predictions)\n\u001b[0;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mword error rate: \u001b[39m\u001b[39m{wer_score:.4f}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Rigobert Kiata\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\jiwer\\measures.py:111\u001b[0m, in \u001b[0;36mwer\u001b[1;34m(reference, hypothesis, reference_transform, hypothesis_transform, truth, truth_transform)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39mCalculate the word error rate (WER) between one or more reference and\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[39mhypothesis sentences.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39m             hypothesis sentence(s).\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     97\u001b[0m (\n\u001b[0;32m     98\u001b[0m     reference,\n\u001b[0;32m     99\u001b[0m     hypothesis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m     hypothesis_transform\u001b[39m=\u001b[39mhypothesis_transform,\n\u001b[0;32m    109\u001b[0m )\n\u001b[1;32m--> 111\u001b[0m output \u001b[39m=\u001b[39m process_words(\n\u001b[0;32m    112\u001b[0m     reference, hypothesis, reference_transform, hypothesis_transform\n\u001b[0;32m    113\u001b[0m )\n\u001b[0;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m output\u001b[39m.\u001b[39mwer\n",
      "File \u001b[1;32mc:\\Users\\Rigobert Kiata\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\jiwer\\process.py:162\u001b[0m, in \u001b[0;36mprocess_words\u001b[1;34m(reference, hypothesis, reference_transform, hypothesis_transform)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mone or more references are empty strings\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    161\u001b[0m \u001b[39m# pre-process reference and hypothesis by applying transforms\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m ref_transformed \u001b[39m=\u001b[39m _apply_transform(\n\u001b[0;32m    163\u001b[0m     reference, reference_transform, is_reference\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    164\u001b[0m )\n\u001b[0;32m    165\u001b[0m hyp_transformed \u001b[39m=\u001b[39m _apply_transform(\n\u001b[0;32m    166\u001b[0m     hypothesis, hypothesis_transform, is_reference\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    167\u001b[0m )\n\u001b[0;32m    169\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(ref_transformed) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(hyp_transformed):\n",
      "File \u001b[1;32mc:\\Users\\Rigobert Kiata\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\jiwer\\process.py:355\u001b[0m, in \u001b[0;36m_apply_transform\u001b[1;34m(sentence, transform, is_reference)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[39mif\u001b[39;00m is_reference:\n\u001b[0;32m    352\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_list_of_list_of_strings(\n\u001b[0;32m    353\u001b[0m         transformed_sentence, require_non_empty_lists\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    354\u001b[0m     ):\n\u001b[1;32m--> 355\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    356\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAfter applying the transformation, each reference should be a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    357\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnon-empty list of strings, with each string being a single word.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    358\u001b[0m         )\n\u001b[0;32m    359\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    360\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_list_of_list_of_strings(\n\u001b[0;32m    361\u001b[0m         transformed_sentence, require_non_empty_lists\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    362\u001b[0m     ):\n",
      "\u001b[1;31mValueError\u001b[0m: After applying the transformation, each reference should be a non-empty list of strings, with each string being a single word."
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "targets = []\n",
    "for batch in test_dataset:\n",
    "   X, y = batch\n",
    "   batch_predictions = model.predict(X)\n",
    "   batch_predictions = decode_batch_predictions(batch_predictions)\n",
    "   predictions.extend(batch_predictions)\n",
    "   for label in y:\n",
    "      label = tf.strings.reduce_join(num_to_char(label)).numpy.decode(\"utf-8\")\n",
    "      targets.append(label)\n",
    "wer_score = wer(targets, predictions)\n",
    "print(\"-\" * 100)\n",
    "print(\"word error rate: {wer_score:.4f}\")\n",
    "print(\"-\" * 100)\n",
    "for i in np.random.randint(0, len(predictions), 5):\n",
    "   print(f\"Target :  {targets[i]}\")\n",
    "   print(f\"Prediction:  {predictions[i]}\")\n",
    "   print(\"-\" * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
